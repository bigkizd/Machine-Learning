{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tập 4\n",
    "Đào Minh Toàn - 1512581"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERALIZATION ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d<sub>vc</sub> = VC dimension, highest number of points shatterable by hypothesis set H (m<sub>H</sub>(N) = 2<sup>N</sup>\n",
    "- &delta; = 4 * m<sub>H</sub>(2N) e<sup>(-N/8)&epsilon;<sup>2</sup></sup>\n",
    "&rArr; ln(&delta;/4) - ln(m<sub>H</sub>(2N)) = (-N/8) * &epsilon;<sup>2</sup> \n",
    "&rArr; (8/N) * (ln (m<sub>H</sub>(2N)) - ln(&delta;/4)) = &epsilon;<sup>2</sup>\n",
    "- For N &gt; d<sub>vc</sub>, use simple approximation m<sub>H</sub>(N) &asymp; N<sup>d<sub>vc</sub></sup>\n",
    "&rArr; (8/N) * (ln ((2N)<sup>d<sub>vc</sub></sup>) - ln(&delta;/4)) = &epsilon;<sup>2</sup>\n",
    "&rArr; (8/N) * ((d<sub>vc</sub> * ln(2N)) - ln(&delta;/4)) = &epsilon;<sup>2</sup>\n",
    "\n",
    "Given d<sub>vc</sub> = 10, &delta; = 0.05 (95% confidence), here are some N and epsilon. Note that N = 460,000 yields an epsilon &le; 0.05. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Câu 1\n",
    "Chọn đáp án D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giải thích cho câu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vc_eps(delta, N, dvc):\n",
    "    return math.sqrt((8.0/N)*((dvc*math.log(2.0*N))-math.log(delta/4.0)))\n",
    "\n",
    "prob1_N = [400000,420000,440000, 460000, 480000]\n",
    "prob1_delta = 0.05\n",
    "prob1_dvc = 10\n",
    "\n",
    "for cur_N in prob1_N:\n",
    "    cur_eps = vc_eps(prob1_delta, cur_N, prob1_dvc)\n",
    "    print(\"N = %d : epsilon = %f\" % (cur_N, cur_eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Câu 2\n",
    "### Đề bài\n",
    "Now we wish to compare bounds on generalization error &epsilon; holding with probability 1 - &delta;. We wish to compare the original VC bound, Rademacher penalty bound, Parrondo and Van den Broek bound, and Devroye bound on d<sub>vc</sub> = 50 and &delta; = 0.05.\n",
    "\n",
    "- Rademacher: &epsilon; &le; sqrt((2/N) * ln(2*N*m<sub>H</sub>(N))) + sqrt((2/N)*ln(1/delta)) + (1/N)\n",
    "- Note that Parrondo and Devroye are **IMPLICIT** bounds and thus N cannot be separated from &epsilon; . Thus a possible solution is to test many &epsilon;s starting from 0 and return the one before the break point (where the inequality fails)\n",
    "\n",
    "\n",
    "From the following plot, we can see that the Devroye bound is the lowest for large N.\n",
    "### Trả lời\n",
    "Chọn đáp án D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giải thích cho câu 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vc_mapper(N):\n",
    "    return vc_eps(0.05, N, 50)\n",
    "\n",
    "vc_vec = np.vectorize(vc_mapper)\n",
    "\n",
    "def rademacher_eps(delta, N, dvc):\n",
    "    cur_eps = math.sqrt((2.0/N)*(math.log(2.0*N)+(dvc*math.log(N)))) + math.sqrt((2.0/N) * math.log(1.0/delta)) + (1.0/N)\n",
    "    return cur_eps\n",
    "\n",
    "def rademacher_mapper(N):\n",
    "    return rademacher_eps(0.05, N, 50)\n",
    "\n",
    "rademacher_vec = np.vectorize(rademacher_mapper)\n",
    "\n",
    "eps_range = np.arange(0.0,10, 0.0001)\n",
    "\n",
    "def parrondo_eps(delta, N, dvc):\n",
    "    last_true = False\n",
    "    last_eps = eps_range[0]\n",
    "    for cur_eps in eps_range:\n",
    "        cur_rightside = (1.0/N) * ((2.0*cur_eps) + math.log(6.0/delta) + (dvc*math.log(2.0*N)))\n",
    "        cur_rightside = math.sqrt(cur_rightside)\n",
    "        cur_true = cur_eps <= cur_rightside\n",
    "        if cur_true == False and last_true == True:\n",
    "            break\n",
    "        elif cur_true == True:\n",
    "            last_true = True\n",
    "            last_eps = cur_eps\n",
    "    return last_eps\n",
    "\n",
    "def parrondo_mapper(N):\n",
    "    return parrondo_eps(0.05, N, 50)\n",
    "\n",
    "parrondo_vec = np.vectorize(parrondo_mapper)\n",
    "\n",
    "def devroye_eps(delta, N, dvc):\n",
    "    last_true = False\n",
    "    last_eps = eps_range[0]\n",
    "    for cur_eps in eps_range:\n",
    "        cur_rightside = (1.0/(2.0*N))*(((4.0*cur_eps)*(1.0+cur_eps)) + math.log(4.0/delta) + ((2.0*dvc)*math.log(N)))\n",
    "        cur_rightside = math.sqrt(cur_rightside)\n",
    "        cur_true = cur_eps <= cur_rightside\n",
    "        if cur_true == False and last_true == True:\n",
    "            break\n",
    "        elif cur_true == True:\n",
    "            last_true = True\n",
    "            last_eps = cur_eps\n",
    "    return last_eps\n",
    "\n",
    "def devroye_mapper(N):\n",
    "    return devroye_eps(0.05, N, 50)\n",
    "\n",
    "devroye_vec = np.vectorize(devroye_mapper)\n",
    "\n",
    "prob2 = {}\n",
    "prob2[\"N\"] = np.arange(5000,15000, 100)\n",
    "prob2[\"vc\"] = vc_vec(prob2[\"N\"])\n",
    "prob2[\"rad\"] = rademacher_vec(prob2[\"N\"])\n",
    "prob2[\"par\"] = parrondo_vec(prob2[\"N\"])\n",
    "prob2[\"dev\"] = devroye_vec(prob2[\"N\"])\n",
    "\n",
    "prob2[\"plot\"] = plt.figure(figsize=(15,10), dpi=80)\n",
    "prob2[\"ax\"] = prob2[\"plot\"].add_subplot(111)\n",
    "prob2[\"ax\"].set_title(\"Generalization Errors Bounds for Large N\")\n",
    "prob2[\"ax\"].set_xlabel(\"N\")\n",
    "prob2[\"ax\"].set_ylabel(\"epsilon\")\n",
    "prob2[\"ax\"].set_yscale(\"log\", basey=10)\n",
    "\n",
    "\n",
    "prob2[\"ax\"].plot(prob2[\"N\"], prob2[\"vc\"], label=\"VC\")\n",
    "prob2[\"ax\"].plot(prob2[\"N\"], prob2[\"rad\"], label=\"Rademacher\")\n",
    "prob2[\"ax\"].plot(prob2[\"N\"], prob2[\"par\"], label=\"Parrondo/Van Den Broek\")\n",
    "prob2[\"ax\"].plot(prob2[\"N\"], prob2[\"dev\"], label = \"Devroye\")\n",
    "\n",
    "prob2[\"ax\"].legend(loc='upper left', frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Câu 3\n",
    "### Bài toán\n",
    "From the following plot, we can see that the Parrondo/Van den Broek bound yields the lowest bounds for small N.\n",
    "### Trả lời:\n",
    "Chọn đáp án C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giải thích cho câu 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob3 = {}\n",
    "prob3[\"N\"] = np.arange(1,50, 1)\n",
    "prob3[\"vc\"] = vc_vec(prob3[\"N\"])\n",
    "prob3[\"rad\"] = rademacher_vec(prob3[\"N\"])\n",
    "prob3[\"par\"] = parrondo_vec(prob3[\"N\"])\n",
    "prob3[\"dev\"] = devroye_vec(prob3[\"N\"])\n",
    "\n",
    "prob3[\"plot\"] = plt.figure(figsize=(15,10), dpi=80)\n",
    "prob3[\"ax\"] = prob3[\"plot\"].add_subplot(111)\n",
    "prob3[\"ax\"].set_title(\"Generalization Errors Bounds for Small N\")\n",
    "prob3[\"ax\"].set_xlabel(\"N\")\n",
    "prob3[\"ax\"].set_ylabel(\"epsilon\")\n",
    "prob3[\"ax\"].set_yscale(\"log\", basey=10)\n",
    "\n",
    "\n",
    "prob3[\"ax\"].plot(prob3[\"N\"], prob3[\"vc\"], label=\"VC\")\n",
    "prob3[\"ax\"].plot(prob3[\"N\"], prob3[\"rad\"], label=\"Rademacher\")\n",
    "prob3[\"ax\"].plot(prob3[\"N\"], prob3[\"par\"], label=\"Parrondo/Van Den Broek\")\n",
    "prob3[\"ax\"].plot(prob3[\"N\"], prob3[\"dev\"], label = \"Devroye\")\n",
    "\n",
    "prob3[\"ax\"].legend(loc='upper left', frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIAS AND VARIANCE\n",
    "\n",
    "Given target function f : [-1, 1] &rarr; &reals; given by f(x) = sin(&pi;x) with input probability distribution uniform on [-1,1], training set with 2 examples &isin; [-1, 1] picked independently, hypothesis picked by learning algorithm that minimizes the squared error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given :** hypothesis h(x) = ax.\n",
    "\n",
    "**Want :** Expected value of h(x) (g-bar(x)), bias, variance.\n",
    "- Note that g-bar(x) &approx; (1/K) * &sum; (k=1; K) g<sup>&Dscr;<sub>K</sub></sup>(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thực hiện của BIAS AND VARIANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_numpairs = 10000\n",
    "bv_shape = (bv_numpairs, 2)\n",
    "bv_hyp = lambda x: math.sin(math.pi * x)\n",
    "bv_hyp_vec = np.vectorize(bv_hyp)\n",
    "bv_xpairs = np.random.uniform(-1,1,(bv_numpairs,1, 2))\n",
    "bv_labels = bv_hyp_vec(bv_xpairs)\n",
    "\n",
    "\n",
    "bv_xaxis = np.arange(-1,1,0.001)\n",
    "bv_xn = bv_xaxis.shape[0]\n",
    "bv_yaxis = bv_hyp_vec(bv_xaxis) \n",
    "\n",
    "bv_h = [{\"w\": np.array([]), \"var_vals\": np.array([]), \"w_avg\": 0, \"idx\": x} for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(bv_numpairs):\n",
    "    cur_data = bv_xpairs[i].T\n",
    "    cur_data_pinv = np.linalg.pinv(cur_data)\n",
    "    cur_labels = bv_labels[i].T\n",
    "    cur_w = np.dot(cur_data_pinv, cur_labels).flatten()\n",
    "    bv_h[1][\"w\"] = np.concatenate((bv_h[1][\"w\"],cur_w),0)\n",
    "\n",
    "bv_h[1][\"w_avg\"] = np.average(bv_h[1][\"w\"])\n",
    "\n",
    "bv_h[1][\"gb_x\"] = np.multiply(bv_h[1][\"w_avg\"],bv_xaxis) #finding where they fall on g-bar(x)=(a_avg)*x\n",
    "bv_h[1][\"bias_avg\"] = np.average(np.square(np.subtract(bv_h[1][\"gb_x\"],bv_yaxis))) #finding (1/N)*sum(g-bar(x)-sin(pi*x))^2\n",
    "bv_h[1][\"gx\"] = np.multiply(np.reshape(bv_h[1][\"w\"], (bv_numpairs, 1)), np.reshape(bv_xpairs, (bv_numpairs, 2)))\n",
    "bv_h[1][\"gbx\"] = np.multiply(bv_h[1][\"w_avg\"], np.reshape(bv_xpairs, (bv_numpairs, 2)))\n",
    "bv_h[1][\"var_vals\"] = np.average(np.square(np.subtract(bv_h[1][\"gx\"], bv_h[1][\"gbx\"])),axis = 1)\n",
    "\n",
    "bv_h[1][\"var_avg\"] = np.average(bv_h[1][\"var_vals\"])\n",
    "\n",
    "print(\"The average hypothesis of form g_bar(x) = ax is given by a = %f\" % bv_h[1][\"w_avg\"])\n",
    "print(\"The average bias is %f\" % bv_h[1][\"bias_avg\"])\n",
    "print(\"The average variance is %f\" % bv_h[1][\"var_avg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Câu 4 \n",
    "Chọn đáp án E\n",
    "Vì như đã tính toán ở trên a = 1.427259 không có đáp án nào trong 4 đáp án còn lại.\n",
    "## Câu 5\n",
    "Chọn đáp án B \n",
    "Vì average bias là sấp sỉ 0.27 nên đáp án gần nhất là 0.3(B).\n",
    "## Câu 6\n",
    "Chọn đáp án A\n",
    "Vì variance bằng sấp sỉ 0.205802 nên đáp án gần nhất là 0.2 (A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Câu 7\n",
    "Now we want to find the least out-of-sample expected errors from multiple hypotheses (including the one we just did). The hypotheses include:\n",
    "0. h(x) = b\n",
    "1. h(x) = ax\n",
    "2. h(x) = ax + b\n",
    "3. h(x) = ax<sup>2</sup>\n",
    "4. h(x) = ax<sup>2</sup> + b\n",
    "\n",
    "\n",
    "The process for training can go much the same as before, modeling our process over the linear regression example, choosing our weights to represent the unknowns a and b, and squaring the variable x when necessary.\n",
    "Chọn đáp án B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giải thích cho câu 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training cho h(x) = b là giao điểm trong mức ý nghĩa bình phương lỗi nhỏ nhất,\n",
    "#muốn chua dif giữa hai điểm\n",
    "bv_h[0][\"w\"] = np.average(bv_labels, axis=2)\n",
    "bv_h[0][\"w_avg\"] = np.average(bv_h[0][\"w\"])\n",
    "\n",
    "#bias and variance\n",
    "bv_h[0][\"bias_avg\"] = np.average(np.square(np.subtract(bv_h[0][\"w_avg\"], bv_yaxis)))\n",
    "                                    \n",
    "bv_h[0][\"var_avg\"] = np.average(np.square(np.subtract(bv_h[0][\"w\"], bv_h[0][\"w_avg\"])))\n",
    "\n",
    "\n",
    "#h[2]============================================================================\n",
    "                                    \n",
    "#with h [2], mỗi tập huấn luyện 2 ví dụ cần thêm dim x0 = 1 cho mỗi ví dụ\n",
    "#w giữ [a, b] giá trị cho h (x) = ax + b\n",
    "#do đào tạo cho mỗi tập dữ liệu 2 elt\n",
    "for i in range(bv_numpairs):\n",
    "    cur_data = bv_xpairs[i].T\n",
    "    cur_num = cur_data.shape[0] \n",
    "    cur_data = np.c_[np.ones(cur_num), cur_data]\n",
    "    cur_data_pinv = np.linalg.pinv(cur_data)\n",
    "    cur_labels = bv_labels[i].T\n",
    "    cur_w = np.dot(cur_data_pinv, cur_labels).flatten()\n",
    "    bv_h[2][\"w\"] = np.concatenate((bv_h[2][\"w\"],cur_w))\n",
    "bv_h[2][\"w\"] = np.reshape(bv_h[2][\"w\"], bv_shape)\n",
    "bv_h[2][\"w_avg\"] = np.reshape(np.average(bv_h[2][\"w\"], axis=0), (1,2))\n",
    "\n",
    "#bias and variance\n",
    "bv_h[2][\"bias_avg\"] = np.average(np.square(np.subtract(bv_yaxis, np.dot(bv_h[2][\"w_avg\"], np.c_[np.ones(bv_xn), bv_xaxis].T))))                             \n",
    "\n",
    "#Tìm variance\n",
    "#cho mỗi bộ dữ liệu, cần tìm giá trị ngoại lệ E(dataset)[(g(dataset,x)-g-bar(x))^2]\n",
    "for i in range(bv_numpairs):\n",
    "    cur_pairs = bv_xpairs[i].T\n",
    "    cur_pairs_mtx = np.c_[np.ones(cur_pairs.shape[0]), cur_pairs].T\n",
    "    cur_w = bv_h[2][\"w\"][i]\n",
    "    cur_w = np.reshape(cur_w, (1, 2))\n",
    "    cur_gx = np.dot(cur_w, cur_pairs_mtx)\n",
    "    cur_gbx = np.multiply(bv_h[2][\"w_avg\"], cur_pairs_mtx)\n",
    "    cur_gdiff = np.square(np.subtract(cur_gx, cur_gbx))\n",
    "    cur_var = np.average(cur_gdiff)\n",
    "    bv_h[2][\"var_vals\"] = np.concatenate((bv_h[2][\"var_vals\"],[cur_var]))\n",
    "\n",
    "bv_h[2][\"var_avg\"] = np.average(bv_h[2][\"var_vals\"])\n",
    "    \n",
    "#h[3]===================================================================\n",
    "    \n",
    "                                    \n",
    "#h(x) = ax^2\n",
    "#Làm traning, tương tự với trường hợp h[1] (h(x) = ax) ngoại trừ các mục nhập bình phương\n",
    "for i in range(bv_numpairs):\n",
    "    cur_data = np.square(bv_xpairs[i].T)\n",
    "    cur_data_pinv = np.linalg.pinv(cur_data)\n",
    "    cur_labels = bv_labels[i].T\n",
    "    cur_w = np.dot(cur_data_pinv, cur_labels).flatten()\n",
    "    bv_h[3][\"w\"] = np.concatenate((bv_h[3][\"w\"],cur_w),0)\n",
    "bv_h[3][\"w_avg\"] = np.average(bv_h[3][\"w\"])\n",
    "\n",
    "#bias and variance\n",
    "bv_h[3][\"bias_avg\"] = np.average(np.square(np.subtract(bv_yaxis, np.multiply(bv_h[3][\"w_avg\"], np.square(bv_xaxis)))))\n",
    "\n",
    "bv_h[3][\"gx\"] = np.multiply(np.reshape(bv_h[3][\"w\"], (bv_numpairs, 1)), np.reshape(np.square(bv_xpairs), (bv_numpairs, 2)))\n",
    "bv_h[3][\"gbx\"] = np.multiply(bv_h[3][\"w_avg\"], np.reshape(np.square(bv_xpairs), (bv_numpairs, 2)))\n",
    "bv_h[3][\"var_vals\"] = np.average(np.square(np.subtract(bv_h[3][\"gx\"], bv_h[3][\"gbx\"])),axis = 1)\n",
    "#now find average variance\n",
    "bv_h[3][\"var_avg\"] = np.average(bv_h[3][\"var_vals\"])\n",
    "\n",
    "#h[4]=========================================================================\n",
    "#h(x) = ax^2 + b\n",
    "#tương tự tập training trong trường hợp h3 với thêm x0=1 chiều nhưng bình phương x là h4\n",
    "for i in range(bv_numpairs):\n",
    "    cur_data = bv_xpairs[i].T\n",
    "    cur_num = cur_data.shape[0] #number of examples\n",
    "    cur_data = np.c_[np.ones(cur_num), np.square(cur_data)]\n",
    "    cur_data_pinv = np.linalg.pinv(cur_data)\n",
    "    cur_labels = bv_labels[i].T\n",
    "    cur_w = np.dot(cur_data_pinv, cur_labels).flatten()\n",
    "    bv_h[4][\"w\"] = np.concatenate((bv_h[4][\"w\"],cur_w))\n",
    "    \n",
    "bv_h[4][\"w\"] = np.reshape(bv_h[4][\"w\"], bv_shape)\n",
    "bv_h[4][\"w_avg\"] = np.average(bv_h[4][\"w\"], axis=0)\n",
    "\n",
    "#bias and variance\n",
    "bv_h[4][\"bias_avg\"] = np.average(np.square(np.subtract(bv_yaxis, np.dot(bv_h[4][\"w_avg\"], np.c_[np.ones(bv_xn), np.square(bv_xaxis)].T))))                             \n",
    "\n",
    "#find the variance\n",
    "#cho mỗi bộ dữ liệu, cần tìm giá trị ngoại lệ E(dataset)[(g(dataset,x)-g-bar(x))^2]\n",
    "for i in range(bv_numpairs):\n",
    "    cur_pairs = bv_xpairs[i].T\n",
    "    cur_pairs_mtx = np.c_[np.ones(cur_pairs.shape[0]), np.square(cur_pairs)].T\n",
    "    cur_w = bv_h[4][\"w\"][i]\n",
    "    cur_w = np.reshape(cur_w, (1, 2))\n",
    "    cur_gx = np.dot(cur_w, cur_pairs_mtx)\n",
    "    cur_gbx = np.multiply(bv_h[4][\"w_avg\"], cur_pairs_mtx)\n",
    "    cur_gdiff = np.square(np.subtract(cur_gx, cur_gbx))\n",
    "    cur_var = np.average(cur_gdiff)\n",
    "    bv_h[4][\"var_vals\"] = np.concatenate((bv_h[1][\"var_vals\"],[cur_var]))\n",
    "\n",
    "bv_h[4][\"var_avg\"] = np.average(bv_h[1][\"var_vals\"])\n",
    "    \n",
    "\n",
    "\n",
    "for hyp in bv_h:\n",
    "    print(\"The expected value of e_out for hypothesis %d is %f\" % (hyp[\"idx\"], hyp[\"bias_avg\"] + hyp[\"var_avg\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VC DIMENSION\n",
    "\n",
    "### Câu 8\n",
    "Chọn đáp án C\n",
    "\n",
    "Giải thích:\n",
    "Cho q &gt; N, NCq = 0, vì vậy cho đến khi chúng tôi đạt đến mức cần thiết N, n<sub>H</sub>(N+1) = 2*m<sub>H</sub>(N) và từ khi cho N = 1, m<sub>H</sub>(N) = 2 = 2<sup>1</sup>, m<sub>H</sub>(N) = 2<sup>N</sup> cho N &le; q. NCq trở thành non-zero trong construction của m<sub>H</sub>(q+1) = 2*M<sub>H</sub>(q) - qCq (since qCq = 1), và khi M<sub>H</sub>(q+1) < 2<sup>q+1</sup>, q+1 là break point and q is the VC dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9\n",
    "Chọn đáp án B\n",
    "\n",
    "Giải thích:\n",
    "Vì chúng **intersection** chúng ta đang xử lý với giả thuyết **common** tới tất cả các bộ giả thuyết. Xem xét bộ giả thuyết với  lowest VC dimension H<sub>low</sub>, với d<sub>VC</sub> = d<sub>low</sub>. Vì **any** giả thuyết từ H<sub>low</sub> có thể bị gãy nhiều nhất d<sub>low</sub> điểm và giao điểm giữa tất cả các bộ giả thuyết phải là bộ con của tất các bộ giả thuyết, tại nhiều giao điểm có thể bị gãy là d<sub>low</sub> điểm and thus 0 &le; d<sub>VC</sub>(&Intersection; (k=1; K) H<sub>k</sub>) &le; min (k=1;K){d<sub>VC</sub>(H<sub>k</sub>)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 10\n",
    "Chọn đáp án E\n",
    "\n",
    "Giải thích:\n",
    "Bây giờ xem xét bộ giả thuyết như trước nhưng **union** thể hiện của intersection. Thay vì giao điểm là tập hợp con của tất cả các bộ như trước chúng ta xử lý với union, chúng ta có thể xem xét mỗi bộ giả thuyết như một bộ con của union. Trong union chúng ta có bộ giả thuyết với VC dimension cao nhất aka max (k=1;K) {d<sub>VC</sub> (H<sub>k</sub>)}. Giả trị này phải lower bound cho VC dimension của union.\n",
    "\n",
    "\n",
    "Upper bound có thể nhiều hơn tổng của mỗi bộ giả thuyết VC dimension (&sum; (k=1;K){d<sub>VC</sub>(H<sub>k</sub>)}). Xem xét hai bộ giả thuyết H1 and H2, y = d1 + d2 + x cho x, d1, d2 ;gt 0. Cho union H12. H1 chứa tất cả các hoán vị của up to và bao gồm d1 -'s với rest +'s (tất cả +'s, y-1 +'s và 1 -, y-2 +'s và 2 -'s, etc.). VC dimension là d1 (VD: Cho sake, Xem xét d1 = 3, d2 = x = 0, then = {+++, ++-, +-+,-++, +--, -+-,+--,+++}), Cho H2 là đối (d2 +'s, rest -'s) do đó với d2 chiều. Đó là disjoint vì có ít nhất d1 + x -'s trong H2 trong khi H1 có tại hầu hết d1. Với H1 có ít nhất d2 + x +'s như tốt,  Xem xét bộ con của d1 + d2 điểm gãy bằng H1 and H2 trong H12. Tacking vào các điểm phụ được đảm bảo bởi x &gt 0 và thực tế rằng nó có thể + or - cho chúng ta một điểm vỡ hơn, do đó kích thước VC của **union** H12 là ít nhất **d1 + d2 + 1** (tín dụng cho MindExodus từ Diễn đàn từ Diễn đàn dữ liệu để chỉ ra lập luận này).Là lựa chọn duy nhất trong bài tập về nhà phản ánh giới hạn này là **max(k=1;K){d<sub>VC</sub>(H<sub>k</sub>)} &le; d<sub>VC</sub>(&Union;(k=1;K){H<sub>k</sub>}) &le; K - 1 + &sum;(k=1;K){d<sub>VC</sub>(H<sub>k</sub>)}**, Đây phải là kết quả chính xác (mặc dù tính chính xác của K - 1 thuật ngữ bổ sung mà tôi đã không thể suy ra, tôi nghi ngờ điều này có thể rơi d1+d2+1 các điểm vỡ được phân tách giữa các cặp công đoàn với các chồng chéo nằm trên cụm từ +1 bổ sung đó vào d1 + d2)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
